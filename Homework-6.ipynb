{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 6\n",
    "(due 11/24)\n",
    "\n",
    "## Image quantization (40 pts)\n",
    "\n",
    "One popular use of the k-means algorithm is in image compression.  In general ,images are encoded as a grid of 3 8-bit integers (the red, green, and blue bands, each ranging from 0-255).  To use K-means to compress an image, K-means clustering is performed on the array of pixel values to obtain the K colors that in some sense contain the most information.  The compressed image is generated by replacing each pixel value with the index of the nearest K-means cluster center.  When it is time to display the image, each pixel value is reconstructed by looking up the RGB value corresponding to its index.  This process is referred to as *Image quantization*.\n",
    "\n",
    "Import an image of your choosing, and apply K-means based image quantization to it.  You can write the code for K-means yourself, adapt the code from the lecture, or you can use an existing implementation (e.g. scikit-learn).\n",
    "\n",
    "**Use $k=2^3=8$ clusters**.  Display both the original image and the quantized version. \n",
    "\n",
    "**Please also answer the following questions.**\n",
    "1. Is the above compression technique \"lossy\", i.e. could you recover the original image from the compressed one?\n",
    "2. (Grad students only) In theory, how much storage savings is achieved using this type of compression compared to an 8-bit, 3-channel uncompressed image? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Components Analysis (40 pts)\n",
    "\n",
    "In class, we explored the concept of eigendigits, which were a more information-rich basis for representing the handwritten digits of the MNIST dataset.  In fact, a similar procedure can be performed for any standardized images dataset.  In this problem, we will find so-called *eigenfaces*, which are pretty much as they sound: the principal components of a face dataset.  The faces that we will use can be found in the so-called 'labelled faces in the wild' data set.  This can be downloaded via the scikit-learn module as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "# Download labelled faces in the wild (only examples for which there are more than 50 examples)\n",
    "lfw = fetch_lfw_people(min_faces_per_person=50, resize=0.7)\n",
    "\n",
    "# Interrogate the data for the size of the images (h,w) \n",
    "m, h, w = lfw.images.shape\n",
    "\n",
    "# For our purposes, as in MNIST, we will use a flattened version of the pixels\n",
    "X = lfw.data\n",
    "n = X.shape[1]\n",
    "\n",
    "y = lfw.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the second problem in this homework deals with classification, let's split the LFW data into a training and test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A\n",
    "Using either your own implementation or the implementation given by scikit-learn, **perform a principal components analysis on the LFW data, retaining sufficient components to explain 95% of the total data variance (10pts).**  To prove that your PCA is successfully capturing this level of variability, **Generate a scree plot showing the cumulative explained variance as a function of number of principal components (10pts).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B\n",
    "**Visualize the first five eigenfaces by reshaping the resulting principal components into images and plotting them (10pts).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C\n",
    "(10pts) Consider the following two datasets (X_1 and X_2), each with three dimensions.  How many principal components do you expect each to have?  How do you know? (You don't actually need to do the PCA to answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = np.array([[ 0.24658525,  0.846718  ,  0.29263623],\n",
    "       [ 1.94365644, -0.78759333,  0.81430956],\n",
    "       [ 0.54530612,  1.33540717,  0.53973449],\n",
    "       [ 0.08287299,  1.41172682,  0.32378186],\n",
    "       [ 1.16505735, -0.76913387,  0.4287019 ],\n",
    "       [ 0.80324671, -0.3969266 ,  0.32223804],\n",
    "       [ 0.30891776, -0.62816207,  0.02882647],\n",
    "       [ 0.68643482,  0.95395446,  0.5340083 ],\n",
    "       [-1.1862806 ,  1.80433744, -0.23227281],\n",
    "       [ 1.31814933, -0.86135592,  0.48680348],\n",
    "       [-0.01355747,  0.52411544,  0.09804436],\n",
    "       [-0.94016758, -1.00530368, -0.67114452],\n",
    "       [-1.53088917, -0.79227508, -0.9238996 ],\n",
    "       [ 0.87683622, -1.29639414,  0.17913928],\n",
    "       [-0.34180964, -0.21053314, -0.21301145],\n",
    "       [-0.40673884, -0.89787012, -0.38294344],\n",
    "       [-0.74792211,  1.42602549, -0.08875596],\n",
    "       [-0.10994822, -1.34930993, -0.3248361 ],\n",
    "       [-0.09104714, -0.87550541, -0.22062465],\n",
    "       [-0.18231387,  0.51312677,  0.01146842],\n",
    "       [ 1.48119305, -0.77899653,  0.58479722],\n",
    "       [ 0.67944609,  0.31732884,  0.40318881],\n",
    "       [ 0.8137745 ,  2.09032765,  0.82495278],\n",
    "       [-0.81678612,  0.9302194 , -0.22234918],\n",
    "       [ 1.32824051,  0.88054246,  0.84022875]])\n",
    "\n",
    "X_2 = np.array([[ 9.97897650e-01,  2.99369295e+00,  4.49053943e-01],\n",
    "       [-9.19396971e-01, -2.75819091e+00, -4.13728637e-01],\n",
    "       [ 2.63408733e-01,  7.90226199e-01,  1.18533930e-01],\n",
    "       [ 1.23229118e-01,  3.69687353e-01,  5.54531030e-02],\n",
    "       [ 1.00365433e+00,  3.01096300e+00,  4.51644450e-01],\n",
    "       [-9.73346396e-01, -2.92003919e+00, -4.38005878e-01],\n",
    "       [-2.48058203e-01, -7.44174610e-01, -1.11626192e-01],\n",
    "       [-1.14257767e+00, -3.42773300e+00, -5.14159949e-01],\n",
    "       [-4.51139403e-01, -1.35341821e+00, -2.03012732e-01],\n",
    "       [-2.01440713e-01, -6.04322138e-01, -9.06483206e-02],\n",
    "       [ 1.15492027e+00,  3.46476081e+00,  5.19714121e-01],\n",
    "       [ 6.86908285e-02,  2.06072486e-01,  3.09108728e-02],\n",
    "       [-4.73749885e-01, -1.42124965e+00, -2.13187448e-01],\n",
    "       [ 1.11404171e+00,  3.34212513e+00,  5.01318770e-01],\n",
    "       [-8.13230322e-01, -2.43969096e+00, -3.65953645e-01],\n",
    "       [-8.70193912e-01, -2.61058174e+00, -3.91587261e-01],\n",
    "       [ 6.55592608e-01,  1.96677783e+00,  2.95016674e-01],\n",
    "       [ 2.88671096e-02,  8.66013288e-02,  1.29901993e-02],\n",
    "       [ 6.64698327e-01,  1.99409498e+00,  2.99114247e-01],\n",
    "       [ 4.83556414e-01,  1.45066924e+00,  2.17600386e-01],\n",
    "       [ 2.74450530e-01,  8.23351591e-01,  1.23502739e-01],\n",
    "       [-2.38147337e-03, -7.14442010e-03, -1.07166302e-03],\n",
    "       [ 1.39721186e+00,  4.19163557e+00,  6.28745336e-01],\n",
    "       [-1.44265778e+00, -4.32797333e+00, -6.49196000e-01],\n",
    "       [-7.39010087e-01, -2.21703026e+00, -3.32554539e-01]])\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(*X_1.T,'k.')\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(*X_2.T,'k.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
